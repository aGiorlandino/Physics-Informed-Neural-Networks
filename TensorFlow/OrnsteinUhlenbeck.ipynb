{"cells":[{"cell_type":"code","source":["!pip install pyDOE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGNG42Iq3mZ-","executionInfo":{"status":"ok","timestamp":1652554400259,"user_tz":-120,"elapsed":2398,"user":{"displayName":"alessio giorlandino","userId":"14669683435891599642"}},"outputId":"cde82211-86b9-43a2-a2a5-52452ee04197"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyDOE in /usr/local/lib/python3.7/dist-packages (0.3.8)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.21.6)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D0uOafiD3qmv","executionInfo":{"status":"ok","timestamp":1652554401717,"user_tz":-120,"elapsed":1465,"user":{"displayName":"alessio giorlandino","userId":"14669683435891599642"}},"outputId":"763db970-532c-46bb-878e-0dcadb428732"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPPr5oba3j88","executionInfo":{"status":"ok","timestamp":1652554401718,"user_tz":-120,"elapsed":9,"user":{"displayName":"alessio giorlandino","userId":"14669683435891599642"}},"outputId":"49607ca4-ace7-4edd-a138-9abe892123e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.8.0\n"]}],"source":["import tensorflow as tf\n","import datetime, os\n","#hide tf logs \n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'} \n","#0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs, and 3 to additionally filter out ERROR logs\n","import scipy.optimize\n","import scipy.io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","from mpl_toolkits.mplot3d import Axes3D\n","import time\n","from pyDOE import lhs         #Latin Hypercube Sampling\n","import seaborn as sns \n","import codecs, json\n","\n","# generates same random numbers each time\n","np.random.seed(1234)\n","tf.random.set_seed(1234)\n","\n","print(\"TensorFlow version: {}\".format(tf.__version__))"]},{"cell_type":"markdown","metadata":{"id":"88G3Lt8xn-Oo"},"source":["# *Data Prep*\n","\n","Training and Testing data is prepared from the solution file"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"HC_Gn7mu3j9B","executionInfo":{"status":"ok","timestamp":1652554401718,"user_tz":-120,"elapsed":6,"user":{"displayName":"alessio giorlandino","userId":"14669683435891599642"}}},"outputs":[],"source":["from scipy.stats import norm\n","#getting collocation points\n","x = np.linspace(-1, 1, 256)                     # 256 points between -1 and 1 [256x1]\n","t = np.linspace(0, 1, 100)                     # 100 time points between 0 and 1 [100x1] \n","usol=np.zeros((256,100))\n","\n","usol[:,0]=norm.pdf(x,0,0.01)\n","\n","#collocation points for every position and every time\n","X, T = np.meshgrid(x,t)"]},{"cell_type":"markdown","metadata":{"id":"ZyGxyaOAcqpi"},"source":["# *Test Data*\n","\n","We prepare the test data to compare against the solution produced by the PINN."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"yddknKA2Xohp","executionInfo":{"status":"ok","timestamp":1652554401718,"user_tz":-120,"elapsed":5,"user":{"displayName":"alessio giorlandino","userId":"14669683435891599642"}}},"outputs":[],"source":["''' X_u_test = [X[i],T[i]] [25600,2] for interpolation'''\n","X_u_test = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n","\n","# Domain bounds\n","lb = X_u_test[0]  # [-1. 0.]\n","ub = X_u_test[-1] # [1.  0.99]\n","\n","'''\n","   Fortran Style ('F') flatten,stacked column wise!\n","   u = [c1 \n","        c2\n","        .\n","        .\n","        cn]\n","\n","   u =  [25600x1] \n","'''\n","u = usol.flatten('F')[:,None] "]},{"cell_type":"markdown","metadata":{"id":"aJ5oBRtEXnyu"},"source":["# *Training Data*\n","\n","The boundary conditions serve as the test data for the PINN and the collocation points are generated using **Latin Hypercube Sampling**"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"8UVJmvZbXjXb","executionInfo":{"status":"ok","timestamp":1652554401961,"user_tz":-120,"elapsed":248,"user":{"displayName":"alessio giorlandino","userId":"14669683435891599642"}}},"outputs":[],"source":["def trainingdata(N_u,N_f):\n","\n","    '''Boundary Conditions'''\n","\n","    #Initial Condition -1 =< x =<1 and t = 0  \n","    leftedge_x = np.hstack((X[0,:][:,None], T[0,:][:,None])) #L1\n","    leftedge_u = usol[:,0][:,None]\n","\n","    #Boundary Condition x = -1 and 0 =< t =<1\n","    bottomedge_x = np.hstack((X[:,0][:,None], T[:,0][:,None])) #L2\n","    bottomedge_u = usol[-1,:][:,None]\n","\n","    #Boundary Condition x = 1 and 0 =< t =<1\n","    topedge_x = np.hstack((X[:,-1][:,None], T[:,0][:,None])) #L3\n","    topedge_u = usol[0,:][:,None]\n","\n","    all_X_u_train = np.vstack([leftedge_x, bottomedge_x, topedge_x]) # X_u_train [456,2] (456 = 256(L1)+100(L2)+100(L3))\n","    all_u_train = np.vstack([leftedge_u, bottomedge_u, topedge_u])   #corresponding u [456x1]\n","\n","    #choose random N_u points for training\n","    idx = np.random.choice(all_X_u_train.shape[0], N_u, replace=False) \n","\n","    X_u_train = all_X_u_train[idx, :] #choose indices from  set 'idx' (x,t)\n","    u_train = all_u_train[idx,:]      #choose corresponding u\n","\n","    '''Collocation Points'''\n","\n","    # Latin Hypercube sampling for collocation points \n","    # N_f sets of tuples(x,t)\n","    X_f_train = lb + (ub-lb)*lhs(2,N_f) \n","    X_f_train = np.vstack((X_f_train, X_u_train)) # append training points to collocation points \n","\n","    return X_f_train, X_u_train, u_train \n"]},{"cell_type":"markdown","metadata":{"id":"dp4nc2S7bwzz"},"source":["# **PINN**\n","\n","Generate a **PINN** of L hidden layers, each with n neurons. \n","\n","Initialization: ***Xavier***\n","\n","Activation: *tanh (x)*"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Ivj5SRpG3j9F","executionInfo":{"status":"ok","timestamp":1652554401961,"user_tz":-120,"elapsed":4,"user":{"displayName":"alessio giorlandino","userId":"14669683435891599642"}}},"outputs":[],"source":["class Sequentialmodel(tf.Module): \n","    def __init__(self, layers, name=None):\n","       \n","        self.W = []  #Weights and biases\n","        self.parameters = 0 #total number of parameters\n","        \n","        for i in range(len(layers)-1):\n","            \n","            input_dim = layers[i]\n","            output_dim = layers[i+1]\n","            \n","            #Xavier standard deviation \n","            std_dv = np.sqrt((2.0/(input_dim + output_dim)))\n","\n","            #weights = normal distribution * Xavier standard deviation + 0\n","            w = tf.random.normal([input_dim, output_dim], dtype = 'float64') * std_dv\n","                       \n","            w = tf.Variable(w, trainable=True, name = 'w' + str(i+1))\n","\n","            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype = 'float64'), trainable = True, name = 'b' + str(i+1))\n","                    \n","            self.W.append(w)\n","            self.W.append(b)\n","            \n","            self.parameters +=  input_dim * output_dim + output_dim\n","    \n","    def evaluate(self,x):\n","        \n","        x = (x-lb)/(ub-lb)\n","        \n","        a = x\n","        \n","        for i in range(len(layers)-2):\n","            \n","            z = tf.add(tf.matmul(a, self.W[2*i]), self.W[2*i+1])\n","            a = tf.nn.tanh(z)\n","            \n","        a = tf.add(tf.matmul(a, self.W[-2]), self.W[-1]) # For regression, no activation to last layer\n","        return a\n","    \n","    def get_weights(self):\n","\n","        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n","        \n","        for i in range (len(layers)-1):\n","            \n","            w_1d = tf.reshape(self.W[2*i],[-1])   #flatten weights \n","            b_1d = tf.reshape(self.W[2*i+1],[-1]) #flatten biases\n","            \n","            parameters_1d = tf.concat([parameters_1d, w_1d], 0) #concat weights \n","            parameters_1d = tf.concat([parameters_1d, b_1d], 0) #concat biases\n","        \n","        return parameters_1d\n","        \n","    def set_weights(self,parameters):\n","                \n","        for i in range (len(layers)-1):\n","\n","            shape_w = tf.shape(self.W[2*i]).numpy() # shape of the weight tensor\n","            size_w = tf.size(self.W[2*i]).numpy() #size of the weight tensor \n","            \n","            shape_b = tf.shape(self.W[2*i+1]).numpy() # shape of the bias tensor\n","            size_b = tf.size(self.W[2*i+1]).numpy() #size of the bias tensor \n","                        \n","            pick_w = parameters[0:size_w] #pick the weights \n","            self.W[2*i].assign(tf.reshape(pick_w,shape_w)) # assign  \n","            parameters = np.delete(parameters,np.arange(size_w),0) #delete \n","            \n","            pick_b = parameters[0:size_b] #pick the biases \n","            self.W[2*i+1].assign(tf.reshape(pick_b,shape_b)) # assign \n","            parameters = np.delete(parameters,np.arange(size_b),0) #delete \n","\n","            \n","    def loss_BC(self,x,y):\n","\n","        loss_u = tf.reduce_mean(tf.square(y-self.evaluate(x)))\n","        return loss_u\n","\n","    def loss_PDE(self, x_to_train_f):\n","    \n","        g = tf.Variable(x_to_train_f, dtype = 'float64', trainable = False)\n","    \n","        cost=1 \n","        sigma2=3\n","\n","        x_f = g[:,0:1]\n","        t_f = g[:,1:2]\n","\n","        with tf.GradientTape(persistent=True) as tape:\n","\n","            tape.watch(x_f)\n","            tape.watch(t_f)\n","\n","            g = tf.stack([x_f[:,0], t_f[:,0]], axis=1)   \n","\n","            z = self.evaluate(g)\n","            p_x = tape.gradient(z,x_f)\n","\n","        p_t = tape.gradient(z,t_f)    \n","        p_xx = tape.gradient(p_x, x_f)\n","\n","        del tape\n","\n","        p=self.evaluate(g)\n","\n","        f = p_t - cost * p - cost * x_f * p_x - sigma2/2*p_xx\n","\n","        loss_f = tf.reduce_mean(tf.square(f))\n","\n","        return loss_f\n","    \n","    def loss(self,x,y,g):\n","\n","        loss_u = self.loss_BC(x,y)\n","        loss_f = self.loss_PDE(g)\n","\n","        loss = loss_u + loss_f\n","\n","        return loss, loss_u, loss_f\n","    \n","    def optimizerfunc(self,parameters):\n","        \n","        self.set_weights(parameters)\n","       \n","        with tf.GradientTape() as tape:\n","            tape.watch(self.trainable_variables)\n","            \n","            loss_val, loss_u, loss_f = self.loss(X_u_train, u_train, X_f_train)\n","            \n","        grads = tape.gradient(loss_val,self.trainable_variables)\n","                \n","        del tape\n","        \n","        grads_1d = [ ] #flatten grads \n","        \n","        for i in range (len(layers)-1):\n","\n","            grads_w_1d = tf.reshape(grads[2*i],[-1]) #flatten weights \n","            grads_b_1d = tf.reshape(grads[2*i+1],[-1]) #flatten biases\n","\n","            grads_1d = tf.concat([grads_1d, grads_w_1d], 0) #concat grad_weights \n","            grads_1d = tf.concat([grads_1d, grads_b_1d], 0) #concat grad_biases\n","\n","        return loss_val.numpy(), grads_1d.numpy()\n","    \n","    def optimizer_callback(self,parameters):\n","               \n","        loss_value, loss_u, loss_f = self.loss(X_u_train, u_train, X_f_train)\n","        \n","        u_pred = self.evaluate(X_u_test)\n","        error_vec = np.linalg.norm((u-u_pred),2)/np.linalg.norm(u,2)\n","        \n","        tf.print(loss_value, loss_u, loss_f, error_vec)"]},{"cell_type":"markdown","metadata":{"id":"bOjuHdzAhib-"},"source":["# *Solution Plot*"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"UWqNuRMLhg4m","executionInfo":{"status":"ok","timestamp":1652554402240,"user_tz":-120,"elapsed":282,"user":{"displayName":"alessio giorlandino","userId":"14669683435891599642"}}},"outputs":[],"source":["def solutionplot(u_pred,X_u_train,u_train):\n","    \n","    fig, ax = plt.subplots()\n","    ax.axis('off')\n","\n","    gs0 = gridspec.GridSpec(1, 2)\n","    gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n","    ax = plt.subplot(gs0[:, :])\n","\n","    h = ax.imshow(u_pred, interpolation='nearest', cmap='rainbow', \n","                extent=[T.min(), T.max(), X.min(), X.max()], \n","                origin='lower', aspect='auto')\n","    divider = make_axes_locatable(ax)\n","    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n","    fig.colorbar(h, cax=cax)\n","    \n","    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n","\n","    line = np.linspace(x.min(), x.max(), 2)[:,None]\n","    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n","    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n","    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n","\n","    ax.set_xlabel('$t$')\n","    ax.set_ylabel('$x$')\n","    ax.legend(frameon=False, loc = 'best')\n","    ax.set_title('$u(x,t)$', fontsize = 10)\n","    \n","    ''' \n","    Slices of the solution at points t = 0.25, t = 0.50 and t = 0.75\n","    '''\n","    \n","    ####### Row 1: u(t,x) slices ##################\n","    gs1 = gridspec.GridSpec(1, 3)\n","    gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n","\n","    ax = plt.subplot(gs1[0, 0])\n","    ax.plot(x,usol.T[25,:], 'b-', linewidth = 2, label = 'Exact')       \n","    ax.plot(x,u_pred.T[25,:], 'r--', linewidth = 2, label = 'Prediction')\n","    ax.set_xlabel('$x$')\n","    ax.set_ylabel('$u(x,t)$')    \n","    ax.set_title('$t = 0.25s$', fontsize = 10)\n","    ax.axis('square')\n","    ax.set_xlim([-1.1,1.1])\n","    ax.set_ylim([-1.1,1.1])\n","\n","    ax = plt.subplot(gs1[0, 1])\n","    ax.plot(x,usol.T[50,:], 'b-', linewidth = 2, label = 'Exact')       \n","    ax.plot(x,u_pred.T[50,:], 'r--', linewidth = 2, label = 'Prediction')\n","    ax.set_xlabel('$x$')\n","    ax.set_ylabel('$u(x,t)$')\n","    ax.axis('square')\n","    ax.set_xlim([-1.1,1.1])\n","    ax.set_ylim([-1.1,1.1])\n","    ax.set_title('$t = 0.50s$', fontsize = 10)\n","    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n","\n","    ax = plt.subplot(gs1[0, 2])\n","    ax.plot(x,usol.T[75,:], 'b-', linewidth = 2, label = 'Exact')       \n","    ax.plot(x,u_pred.T[75,:], 'r--', linewidth = 2, label = 'Prediction')\n","    ax.set_xlabel('$x$')\n","    ax.set_ylabel('$u(x,t)$')\n","    ax.axis('square')\n","    ax.set_xlim([-1.1,1.1])\n","    ax.set_ylim([-1.1,1.1])    \n","    ax.set_title('$t = 0.75s$', fontsize = 10)\n","    \n","    plt.savefig('Burgers.png',dpi = 500)   "]},{"cell_type":"markdown","metadata":{"id":"YRuuEXx-eeWa"},"source":["# *Model Training and Testing*\n","\n","A function '**model**' is defined to generate a NN as per the input set of hyperparameters, which is then trained and tested. The L2 Norm of the solution error is returned as a comparison metric"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WyH2oLRH3j9K","outputId":"5eb4befd-e44b-43c2-f143-32e0079e5c9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n","3.9703182679586977 3.94950682146095 0.020811446497747663 1.013986516928883\n","3.9648740479300781 3.948963752509187 0.015910295420891341 1.0236077056128945\n","3.9550612999883383 3.9487287001057738 0.0063325998825643189 1.0793329015178035\n","3.9537553624562056 3.9474759550015968 0.0062794074546089051 1.082611221717073\n","3.945801741835929 3.9389049611139506 0.0068967807219784032 1.1005696269132155\n","3.9339538060579975 3.9271396058541495 0.0068142002038478894 1.1308826987583143\n","3.9251978086855046 3.91977971057326 0.0054180981122444816 1.1352916947866174\n","3.9200049972703246 3.9139612266106458 0.00604377065967885 1.1302384508584624\n","3.9162091403257397 3.9093095872886852 0.0068995530370547439 1.1298847680811155\n","3.9146089421411427 3.9073398800248409 0.0072690621163019531 1.1272962145060503\n","3.9119534634116793 3.9031246781562849 0.008828785255394286 1.12495453690241\n","3.9092574883244415 3.9002894080921426 0.008968080232299 1.1302671403131446\n","3.9058408664715478 3.8965708554443852 0.0092700110271627129 1.1207154810984261\n","3.9019464662907031 3.8970398194389628 0.0049066468517401009 1.165363888465673\n","3.8985875275501964 3.8937368870471123 0.0048506405030839589 1.1381901117338071\n","3.8963937488173181 3.8917153811016432 0.0046783677156748372 1.1315514988296351\n","3.8925830709723819 3.8863355625189433 0.0062475084534383872 1.1284769456116361\n","3.8904191077886963 3.8813613445565265 0.0090577632321696567 1.1499793844452095\n","3.8892385280966524 3.8781381496479868 0.011100378448665774 1.1494085454395073\n","3.8879912721608778 3.8759154903419795 0.012075781818898391 1.1571664342152914\n","3.8857697633733688 3.8739362893449818 0.011833474028386909 1.1666675816330307\n","3.8813984074380863 3.8652903071192628 0.016108100318823527 1.1616405752466474\n","3.8792301145221382 3.8652540052245183 0.013976109297619915 1.156859708306534\n","3.8773427241574723 3.8640907789590568 0.013251945198415287 1.1377448760029212\n","3.8755022578736886 3.86135250259879 0.014149755274899024 1.1173862024113053\n","3.8745256082870219 3.8600186674383679 0.01450694084865395 1.1140124754745961\n","3.8728919066269443 3.8570775223362346 0.015814384290709624 1.1109561679194428\n","3.8703332368373622 3.8556323332617395 0.014700903575622641 1.111342245772413\n","3.8657808436540226 3.8496532919679827 0.016127551686039909 1.1056701022352686\n","3.8616592687510849 3.848876278014139 0.012782990736945936 1.1149090846360183\n","3.8575007775978678 3.8454610405665979 0.012039737031269933 1.1328041483278066\n","3.855698281121783 3.8432409371463292 0.012457343975453868 1.1374378784577863\n","3.8552686589395084 3.8443187969499797 0.010949861989528591 1.1421770777703355\n","3.8546110075088023 3.8447505211651052 0.0098604863436968972 1.1418576857124292\n","3.853530536846693 3.8437336673002949 0.0097968695463980637 1.1234338858912176\n","3.85197206374926 3.8426435153537408 0.0093285483955191286 1.1237291572348627\n","3.849509074697826 3.8408688924051289 0.0086401822926969775 1.1186324989920262\n","3.8450625912365162 3.836303354039091 0.0087592371974252025 1.1052674615568034\n","3.8436494134414922 3.8302025304310172 0.013446883010475105 1.1097296126358984\n","3.8411054029266758 3.8288671290441338 0.012238273882541735 1.1051870982126988\n","3.8406742474092432 3.8301968843983136 0.010477363010929658 1.131314470285922\n","3.8389473843663748 3.8293485887787524 0.0095987955876223469 1.1228646575165777\n","3.838114802837544 3.8271853911234275 0.010929411714116372 1.1196258565467898\n","3.8367194773906763 3.8265616298679692 0.010157847522707195 1.1192033764548066\n","3.8329763975352575 3.8228823518822059 0.010094045653051557 1.1305202118836688\n","3.8319208399670153 3.8221552382801507 0.00976560168686453 1.1185556648121784\n","3.8305237671100154 3.8201975160488915 0.010326251061123988 1.1176396895475869\n","3.8287707784510063 3.8131113829246197 0.015659395526386736 1.1115718588746837\n","3.8262159800240885 3.8100202905165204 0.016195689507568084 1.1174325265837692\n","3.8245005404490211 3.8054444654032227 0.019056075045798614 1.1094952697713534\n","3.8233005224788474 3.8030875492212726 0.020212973257574946 1.1045026285595867\n","3.8227259576658583 3.80399311373735 0.018732843928508217 1.1077414878206269\n","3.8220343797090948 3.8019421390637094 0.020092240645385357 1.1088859607597437\n","3.8206285184140327 3.8003379051810953 0.020290613232937609 1.10876173174954\n","3.8176856493163394 3.7972827447501576 0.020402904566181698 1.104890277845992\n","3.8158251340319405 3.79611551263254 0.019709621399400724 1.0906491766556043\n","3.8142556023352792 3.7962520962877955 0.018003506047483631 1.0927947231114947\n","3.8129427471873307 3.7942759621576272 0.018666785029703446 1.0917890258346132\n","3.8118654819456212 3.7929975013713744 0.018867980574246945 1.087478033238694\n","3.8092139215955654 3.7898234105861217 0.019390511009443764 1.0889007502046408\n","3.8059433000429643 3.7865583136221654 0.019384986420798778 1.0912960614375147\n","3.8027994222453976 3.7860788528734912 0.016720569371906278 1.1087557216168273\n","3.80108189396201 3.7861238094108618 0.01495808455114846 1.1100346757251986\n","3.7988082842227122 3.7845186059948213 0.014289678227891118 1.1047847528869692\n","3.7983568476325407 3.7842850187388546 0.014071828893685965 1.1034999317102936\n","3.7975028208870376 3.7837261686262549 0.013776652260782517 1.1047445078943816\n","3.7972965992384684 3.7837726941624648 0.01352390507600372 1.094848154874849\n","3.7969606249917605 3.7844037991762041 0.012556825815556304 1.0966419415045296\n","3.7964911362745659 3.7841955009858825 0.012295635288683368 1.0951499585275932\n","3.7956838272808655 3.7840667999189579 0.011617027361907738 1.093704126576867\n","3.7922973025075866 3.7810594944938476 0.011237808013738993 1.089763090617122\n","3.78907056650496 3.7753566345944716 0.0137139319104885 1.0944151283652568\n","3.7874872988794479 3.7750931914645487 0.01239410741489929 1.0984272969774258\n","3.7857884325703206 3.7726781037044592 0.013110328865861378 1.1039702778136284\n","3.7846045449603731 3.770163758397592 0.014440786562781041 1.1088198779492318\n","3.7836373825344034 3.7698744885388447 0.013762893995558639 1.1093128617266184\n","3.7831596987978751 3.7712996586010847 0.011860040196790352 1.109863275995148\n","3.7829748967915977 3.7702873715078793 0.01268752528371824 1.1130061751418483\n","3.7818796001614445 3.7700753168852685 0.011804283276175822 1.105337034381421\n","3.7812722669864458 3.7696946284050061 0.011577638581439792 1.1070443283631686\n","3.7798557917313946 3.7690665791072182 0.010789212624176288 1.1094122089352585\n","3.7780025241303936 3.7677650025314318 0.010237521598961859 1.1171485135090242\n","3.7761697543995307 3.7662298833402406 0.0099398710592900272 1.1226531391646224\n","3.7752927005359043 3.764961795692821 0.010330904843083227 1.1210708681743018\n","3.774448256238093 3.7642079272445397 0.010240328993553163 1.1158978832401778\n","3.7730618214681733 3.7637373031672685 0.0093245183009046922 1.1139992519941864\n","3.7708942742094984 3.7601840413299925 0.01071023287950598 1.1100002458023797\n","3.7700476322611185 3.75510381167278 0.014943820588338147 1.1104046865806987\n","3.7678573537815021 3.7546726258564092 0.013184727925092858 1.1071611316774097\n","3.7667675796551814 3.7527658367257084 0.014001742929473154 1.1018391090561332\n","3.7644839477007523 3.7499618051884367 0.014522142512315643 1.1039825291683563\n","3.7636804521538219 3.7458669741907782 0.017813477963043715 1.110451561391473\n","3.7625393759609711 3.7450374250221397 0.017501950938831588 1.1160041541173191\n","3.7607337374377892 3.7465045516379227 0.014229185799866607 1.1126327016640598\n","3.75891265485553 3.7387128136054577 0.020199841250072261 1.131074263170019\n","3.7584286324889074 3.7364948706210495 0.021933761867857959 1.1376525915673295\n","3.757300929087124 3.7368519819278903 0.020448947159233708 1.1301601656758162\n","3.7564699828792767 3.7376509143678907 0.018819068511385911 1.1287291016401282\n","3.7557146356291247 3.7335381387458622 0.022176496883262353 1.1320293710356313\n","3.7552282404885546 3.7327854275332597 0.022442812955294943 1.1369464862321226\n","3.7542140592259461 3.7339411520260524 0.020272907199893917 1.128834626820312\n","3.7535603716042081 3.7319565955301668 0.021603776074041334 1.1248330756431786\n","3.7526064840604261 3.7312931512531464 0.021313332807279827 1.1228273544746459\n","3.75134545337453 3.7306977132297909 0.020647740144738934 1.1218013010907983\n","3.7498563756929086 3.7274368727092662 0.02241950298364264 1.1249364569206566\n","3.7474148073419067 3.7210881637324316 0.0263266436094749 1.1355088668674027\n","3.7452237179530483 3.7171264215732363 0.02809729637981194 1.1435188118389845\n","3.7431166089330206 3.714629865027256 0.028486743905764417 1.1412368026087545\n","3.7403581319797916 3.7161585026919295 0.024199629287862234 1.1281401496663177\n","3.7394172784188804 3.713501954653414 0.025915323765466482 1.1255833263575015\n","3.7383043515833219 3.7167409419241775 0.021563409659144266 1.1198313099982469\n","3.7377451983360048 3.7148906604454468 0.022854537890557983 1.1217696107922968\n","3.7370155586121143 3.713753820444964 0.02326173816715036 1.1194827088396075\n","3.7345826665701183 3.7097097309569831 0.024872935613135264 1.10723554901788\n","3.733015734235233 3.7073376180969664 0.025678116138266767 1.0971661816569007\n","3.7291075059850769 3.7048260662769064 0.024281439708170403 1.1023814913693595\n","3.7262159420616392 3.7032541469653153 0.022961795096323952 1.1134681448790218\n","3.7240747322402479 3.6975979933224767 0.026476738917771088 1.1262660226630437\n","3.7231676156827067 3.6950055329518512 0.02816208273085568 1.1348569934229953\n","3.7223883613584858 3.6973192734598133 0.025069087898672605 1.1286987026551352\n","3.721359460375131 3.6949571772347394 0.026402283140391808 1.1191919601922327\n","3.721012262261095 3.6950798710474646 0.025932391213630272 1.1167816891185043\n","3.7205676464642394 3.6934798867425935 0.027087759721645992 1.1133413476257994\n","3.7194306582443959 3.6917099574742469 0.027720700770149063 1.1061361611230491\n","3.7173276558723316 3.6883541222125236 0.028973533659807975 1.0955892642109795\n","3.71471434292984 3.6846555877083453 0.030058755221494482 1.0963530461791509\n","3.7082103128280681 3.6783102879324456 0.029900024895622664 1.1003879853053111\n","3.7067373640815204 3.6730388051535372 0.03369855892798318 1.1239107432058835\n","3.7066331762823053 3.6755687232061875 0.031064453076117576 1.1338721388130844\n","3.7047337116367083 3.6806762846912635 0.024057426945444615 1.117317233473014\n","3.7020417448508303 3.6740786490873552 0.027963095763475 1.121127780104749\n","3.6999751017025919 3.6711336483418666 0.028841453360725428 1.1289087703699983\n","3.6967417168569932 3.6691680944550367 0.027573622401956592 1.129919483891003\n","3.6925596555298497 3.6663090655542039 0.026250589975645708 1.13192660500445\n","3.69011370842102 3.6608687517693888 0.029244956651630887 1.1260897912930476\n","3.6888664073656754 3.6611822415931168 0.027684165772558549 1.1158647626671174\n","3.6876006968819204 3.656762449724908 0.030838247157012312 1.1124909116023831\n","3.6855268500211018 3.6531505077325233 0.032376342288578636 1.111778354323874\n","3.6839931892800495 3.6503369709994251 0.033656218280624414 1.1047406415739434\n","3.68226553087256 3.646860902698235 0.035404628174324958 1.0995507815624894\n","3.6795385632865507 3.6406907250145819 0.038847838271968888 1.0931770603314512\n","3.6757428337162281 3.6306131192735562 0.045129714442671887 1.0885640920978\n","3.6725625324606583 3.6237355295741303 0.048827002886528144 1.0878164598276159\n","3.6704415452006987 3.6301498851736547 0.040291660027043895 1.093836291086498\n","3.6685065588866559 3.6267545925374725 0.041751966349183434 1.1331205981759755\n","3.6661545236667425 3.6250718308105059 0.041082692856236849 1.1332675883693064\n","3.6644583185487041 3.6323124887281391 0.032145829820565214 1.1204376107880691\n","3.6618452848850653 3.6266206909583052 0.035224593926760306 1.1135875933460402\n","3.6611674613943572 3.6261882566322203 0.034979204762137082 1.1214769964540126\n","3.6599863913209068 3.6242306232291623 0.0357557680917444 1.1246432524067067\n","3.6591381495741646 3.6253282016728114 0.033809947901353221 1.120517889470704\n","3.658176744006195 3.62424126177561 0.033935482230584804 1.117519920016076\n","3.6563239272994843 3.6218997790701155 0.034424148229368845 1.1185696941668006\n","3.650231124403776 3.6163309994405495 0.033900124963226641 1.1205183272728216\n","3.6463366555551548 3.6093942542167747 0.036942401338380011 1.1193557598138262\n","3.6434417400021961 3.6051991572128177 0.038242582789378372 1.1130094794491352\n","3.6425159901355371 3.6045156739264623 0.038000316209074875 1.1087212056740652\n","3.6414366485667937 3.6034950678254085 0.037941580741385 1.1124275229226879\n","3.63903845150089 3.59977602662743 0.039262424873459711 1.1178679894959593\n","3.6372346722392028 3.5942224155992686 0.043012256639934271 1.1023114322808725\n","3.6364737060589936 3.5957951688590155 0.040678537199978 1.0879144295303358\n","3.6353173603342612 3.5955084125258852 0.039808947808376 1.0859320092954348\n","3.634456517544705 3.5942951593495382 0.040161358195166731 1.0881014729415175\n","3.6337071001356218 3.5898799724988311 0.043827127636790877 1.0867525483447265\n","3.6329040776338517 3.5918621147693752 0.041041962864476408 1.0847235659738397\n","3.6319147023926344 3.5910776163723175 0.040837086020316833 1.084097938992774\n","3.6310846503391878 3.5885878198429024 0.042496830496285519 1.0848267252275412\n","3.6302899753204323 3.5893310730287307 0.040958902291701772 1.088863683427105\n","3.6291111536112224 3.5894344154812745 0.03967673812994793 1.0894963920952359\n","3.6285166745442403 3.5879070217150217 0.040609652829218613 1.092481915781126\n","3.6279460998777866 3.5873196173378425 0.040626482539944164 1.0905895394237555\n","3.627324293261712 3.5851644966348486 0.0421597966268635 1.0882929050709984\n","3.6266617394587404 3.5839250380769987 0.042736701381741744 1.0869877475118803\n","3.6258195173826162 3.5829561886817936 0.042863328700822857 1.0846903639055243\n","3.6244142931928658 3.5834125213102554 0.041001771882610144 1.083755102839536\n","3.62172236182116 3.5838418357989763 0.037880526022183662 1.087760585270043\n","3.6183102535583207 3.5834139243255718 0.034896329232748935 1.10132561167535\n","3.6170262093997669 3.5855805356158088 0.031445673783958149 1.1094281671820745\n","3.616081433124958 3.5814910820889718 0.03459035103598631 1.1025837769079672\n","3.615507794443233 3.5802160958169282 0.03529169862630481 1.099804176609814\n","3.6148761085707837 3.57876354637318 0.036112562197603858 1.0945833114975356\n","3.6139530827596889 3.576980484802025 0.036972597957664048 1.0869295738522329\n","3.6133556212170297 3.5768265155719492 0.036529105645080689 1.0871638770153293\n","3.612774200035612 3.5764266985233615 0.036347501512250663 1.0886133103926847\n","3.6121071034042731 3.5760643803612497 0.036042723043023313 1.0877951022865422\n","3.6107269186459856 3.57426592146242 0.036460997183565491 1.0868867047405053\n","3.6093997898234491 3.5739508640048117 0.035448925818637678 1.0856046408552618\n","3.607094323897555 3.570035780789949 0.037058543107606048 1.0815119783368252\n","3.6040634588865639 3.5643021274514428 0.03976133143512086 1.0789191575509556\n","3.6021911029609948 3.5482983090400109 0.053892793920983874 1.0643165852007417\n","3.5988768999996359 3.5503086180138768 0.04856828198575925 1.0585833715090625\n","3.5969467369169243 3.5530692457372437 0.043877491179680768 1.0591988585394663\n","3.5954778392171405 3.544454392017522 0.0510234471996184 1.0526913890555272\n","3.5936535373843128 3.5388807781323193 0.054772759251993715 1.048085499724691\n","3.5922674499198508 3.5332877150747337 0.05897973484511701 1.0467969802920296\n","3.591776882713154 3.5297428307581833 0.062034051954970477 1.0451031448977781\n","3.59123855848513 3.52656287622097 0.064675682264159609 1.0444693984950861\n","3.590917783987444 3.5261275598298618 0.064790224157582038 1.0489537976923806\n","3.5904019610553259 3.5245714605865293 0.0658305004687968 1.0538460057565397\n","3.5900156278734379 3.5257100042364584 0.064305623636979531 1.056202629790881\n","3.5888992325331115 3.5248597840065616 0.064039448526549678 1.0529904673378698\n","3.5836956519287404 3.5162722038369219 0.067423448091818577 1.0458255920435122\n","3.5818396858386676 3.5149321590792124 0.066907526759455341 1.043981210225217\n","3.5803504424988186 3.5144363462641546 0.065914096234664168 1.0356666635840055\n","3.5793122400747697 3.5146138799111424 0.064698360163627219 1.034028084652296\n","3.5787273841983898 3.5111967195856937 0.067530664612696051 1.0305690203746978\n","3.5781334308683066 3.5142280821297471 0.063905348738559284 1.0275584991147364\n","3.5764780811138146 3.5131283813447016 0.06334969976911306 1.026053147896982\n","3.5751662539129989 3.5076470570751 0.06751919683789856 1.0274288254906618\n","3.5721345321037434 3.5080697983401627 0.064064733763580517 1.0294293324925314\n","3.5686648281574156 3.5071190678316078 0.061545760325807733 1.0270468636074224\n","3.5658067163779514 3.5078864415783886 0.057920274799562736 1.0370091278136235\n","3.5635971758707345 3.5077487521249875 0.055848423745747049 1.0351566769913514\n","3.5618852059285318 3.5050833489986064 0.056801856929925225 1.0372768552334843\n","3.5596030198352073 3.503245569435927 0.056357450399280529 1.0392533665599548\n","3.5571656545569539 3.4939358546303469 0.063229799926606892 1.0339266351529173\n","3.5553871844649354 3.4965597719241841 0.058827412540751439 1.025201642792589\n","3.5516776712631621 3.4956662080985463 0.056011463164615853 1.034922714116873\n","3.5501733165617084 3.4850901730104131 0.065083143551295464 1.023965000169683\n","3.5462397782546073 3.4857730610988922 0.060466717155714958 1.0194264026471087\n","3.5437680319569353 3.4799688764102439 0.063799155546691508 1.0174269326229668\n","3.5415705987510422 3.4732760681340982 0.068294530616944224 1.011085197711802\n","3.5412757324406989 3.4690694165833365 0.072206315857362341 1.0103557827860923\n","3.5402937721048 3.4728239060842117 0.067469866020588062 1.0165376425331045\n","3.5391034562780108 3.469060442497232 0.070043013780779012 1.0244659960128133\n","3.5379055163185722 3.4745456897105216 0.063359826608050687 1.0192287618168652\n","3.5371947783281756 3.4660204867439353 0.071174291584240587 1.0118242179801356\n","3.5359547708696355 3.4635158383153932 0.072438932554242441 1.0184800763785906\n","3.5347711203488061 3.4707280593993644 0.064043060949441788 1.0253719559942713\n","3.5329224820521765 3.46676356507539 0.066158916976786677 1.0208973481668917\n","3.5298193395521738 3.4548376053593364 0.074981734192837476 1.015474241850912\n","3.5248180261454349 3.4526555956959082 0.072162430449526568 1.0181847409379876\n","3.5217241146878515 3.4508068863656427 0.070917228322208939 1.0236068126373181\n","3.5183367226125415 3.4429534804301865 0.075383242182355009 1.0286951222467446\n","3.5155623435495151 3.4434446433878736 0.072117700161641665 1.033074954000665\n","3.5136364277076795 3.4435677862126211 0.0700686414950586 1.0378711077553318\n","3.5121354175934734 3.4389340624343907 0.073201355159082493 1.0416850475058113\n","3.5115176141285582 3.4366778658414723 0.074839748287085728 1.056478280982183\n","3.5079542507986043 3.4365218560717778 0.0714323947268263 1.0532329007853982\n","3.50600989322864 3.4364682004324516 0.069541692796188284 1.047830439208938\n","3.5032038390584606 3.4321607061276351 0.071043132930825578 1.0500329525856058\n","3.4998300160023845 3.4267266272684798 0.073103388733904553 1.0439319450181104\n","3.4986620314437378 3.4283780054682369 0.070284025975501055 1.0416789818906194\n","3.4970579648846765 3.4243684659907658 0.072689498893910687 1.045148321329087\n","3.4964493726516221 3.4222370257529104 0.074212346898711729 1.0459033494071779\n","3.4954881754441773 3.4235786216618367 0.071909553782340777 1.0428866509288415\n","3.4943020395167186 3.4211990111732096 0.073103028343509041 1.0366993447058301\n","3.4928291839986505 3.4186166837033234 0.074212500295327027 1.0305834254430373\n","3.4917453557839742 3.419250187769054 0.072495168014920147 1.027084241423949\n","3.4907873076377949 3.4149313694814909 0.0758559381563041 1.0258310972273934\n","3.4896953627197829 3.4183960532135735 0.071299309506209219 1.0208859955672644\n","3.489015201531187 3.4192793187497896 0.06973588278139721 1.0194592431248242\n","3.4881956938902547 3.4183911372649032 0.069804556625351538 1.0137208826987365\n","3.4871099098371809 3.4152174714227344 0.071892438414446744 1.0098942959693653\n","3.4844966729070972 3.4041896798180886 0.080306993089008377 1.0028208900840212\n","3.4834595884346031 3.4003976441222044 0.08306194431239855 1.001429621894192\n","3.4818999274442723 3.3968992294181994 0.085000698026072879 1.0009050986183612\n","3.4799432324821384 3.3980134436724789 0.081929788809659509 1.005488028284213\n","3.4776623436904393 3.3948781497328686 0.082784193957570418 1.0133631438753399\n","3.4763346318870334 3.3937589724155708 0.0825756594714627 1.0147545879990463\n","3.4738786267590478 3.3925583922629268 0.081320234496121038 1.0109359359061714\n","3.4727506786736586 3.39354793259297 0.079202746080688766 1.0063853656765447\n","3.4708617204510337 3.3920941031793523 0.078767617271681217 1.0089029262982798\n","3.4689160671239838 3.391721623692908 0.077194443431075954 1.0055817674716243\n","3.4659540445585066 3.3879473543944623 0.078006690164044323 0.9986865765871943\n","3.4623164882673714 3.3904003371050897 0.071916151162281852 0.988041248095564\n","3.4588954057038581 3.3873966695719253 0.071498736131932764 0.9873147666085641\n","3.4579849572496832 3.3850470667721475 0.0729378904775359 0.9886164298675644\n","3.4562246513492259 3.3780086292436637 0.078216022105562291 0.9905679628962238\n","3.454645444356407 3.3762041159145197 0.078441328441887226 0.9881273025230393\n","3.4536996746143389 3.37655499184681 0.077144682767528949 0.9879895623543271\n","3.4526972746474778 3.3699235125736418 0.082773762073836021 0.9869490766054105\n","3.4522100070831545 3.364893910558143 0.087316096525011419 0.9875214441106844\n","3.4519541347102169 3.365081289042172 0.086872845668045 0.9873979166106762\n","3.4517048477845815 3.3643693002611674 0.087335547523413881 0.986743163691004\n","3.4512588250349356 3.3619549464724874 0.089303878562448147 0.9858323028666164\n","3.4503980463488442 3.3585507833445547 0.091847263004289345 0.9845291297449553\n","3.4491518986333749 3.3517801970828112 0.0973717015505639 0.9840996472919284\n","3.4467943582747438 3.3463685342520737 0.10042582402266989 0.985886943190295\n","3.4449368598737262 3.3434038044628882 0.10153305541083824 0.9856049899382959\n","3.4429610774029271 3.3430583995860643 0.099902677816862864 0.9914356545666883\n","3.44144292782917 3.34404747016123 0.097395457667939611 0.9927890481177541\n","3.4407728297280409 3.3423710990889051 0.098401730639135973 0.9910793876784932\n","3.439956163038254 3.3406443381976318 0.099311824840622218 0.9941913489218576\n","3.4392385919872446 3.3425908665223245 0.096647725464920134 0.9977420659791183\n","3.4385499808019624 3.3402067437314424 0.0983432370705201 0.9968627405380034\n","3.43796762135045 3.3361634109416913 0.10180421040875885 0.9977857305197169\n","3.4377035625497157 3.3335216991408436 0.10418186340887212 0.9969235819067075\n","3.4375678718198008 3.3328841092048158 0.10468376261498494 0.9972279967902242\n","3.4372554336825356 3.3333915320014489 0.10386390168108657 0.9970363272494231\n","3.4370910314730305 3.3335177652638657 0.10357326620916464 0.9961962995721454\n","3.4367013950906125 3.3322758410461528 0.10442555404445944 0.9939893651744734\n","3.4357900846616496 3.3298019180331049 0.10598816662854452 0.9904666954234341\n","3.4348806424361085 3.3263715816126638 0.10850906082344486 0.9874744043971325\n","3.43315359843351 3.3213185443276245 0.11183505410588582 0.9863468240329282\n","3.4312171722155362 3.3158023509706887 0.11541482124484775 0.9875708615633162\n","3.4299524350841546 3.3154157917732459 0.11453664331090853 0.9914198471739681\n","3.4292939504208517 3.3157207830134952 0.11357316740735657 0.9943148324476871\n","3.427774078145585 3.3198640473004657 0.10791003084511921 1.0002084358749064\n","3.4270116408844178 3.3220698679300642 0.10494177295435342 1.002993563282035\n","3.4266154451152695 3.3228971492568333 0.10371829585843614 1.0024992215958775\n","3.4262557692878555 3.3225193699835933 0.10373639930426196 1.00247330214169\n","3.4259107629619447 3.3216580657893893 0.1042526971725555 1.0038735477362866\n","3.4252595958223626 3.32037864868989 0.10488094713247234 1.0053928970883714\n","3.4233167660292141 3.317128164145795 0.1061886018834191 1.0109435629435752\n","3.4198795736700838 3.3098462220681522 0.11003335160193163 1.0176369184750098\n","3.4164426569885551 3.3032608316788266 0.11318182530972837 1.0322195619068883\n","3.414808451812827 3.2915457783068804 0.12326267350594654 1.0334284300911862\n","3.4125708297288533 3.3038439860764339 0.10872684365241941 1.0207521445382142\n","3.411159617013801 3.3009737861380386 0.11018583087576229 1.0173838487535176\n","3.4100808482580529 3.2977654113726969 0.11231543688535585 1.0193366316492856\n","3.4097781528043516 3.2989169900333684 0.11086116277098321 1.0197202562862573\n","3.4094310048434573 3.2966878978420522 0.11274310700140491 1.0200506908190712\n","3.4086098142259682 3.2937520950880548 0.11485771913791357 1.0194056663756303\n","3.4074702226299332 3.290937478226549 0.1165327444033841 1.013120689745961\n","3.4062929299827589 3.2905777801070712 0.11571514987568757 1.0112035752383868\n","3.4054897275033365 3.2908364174959366 0.11465331000739977 1.009977227794297\n","3.4039574213169894 3.2908014166070636 0.11315600470992582 1.0060211536374608\n","3.4033320314842408 3.2901388419839246 0.11319318950031643 1.0090235794708142\n","3.4022342236878287 3.2878524016223207 0.11438182206550782 1.0102735924476258\n","3.4005217979231253 3.2826937475191738 0.11782805040395128 1.0095466228727068\n","3.3990377352759538 3.2802940025349945 0.11874373274095924 1.0076482625205414\n","3.3977915465504918 3.2828972793894504 0.11489426716104127 1.0074646290487785\n","3.3969753843313941 3.2919748664255821 0.10500051790581212 1.0147842826962978\n","3.394565818980408 3.2851171690175107 0.10944864996289738 1.0110791966742179\n","3.3930557647711592 3.281885845316681 0.11116991945447818 1.0029800029494091\n","3.3896614727855665 3.2779170876675545 0.11174438511801185 1.006950784896437\n","3.3833865482877186 3.26757161985435 0.1158149284333687 1.0164652908868193\n","3.37967821366379 3.2688506251106646 0.11082758855312536 1.018014734223709\n","3.3783719521038034 3.2650599143989707 0.11331203770483284 1.0176096237159244\n","3.3776490903763619 3.2628705292946787 0.11477856108168336 1.016709698508161\n","3.3774470370493117 3.2588956061262939 0.11855143092301777 1.0166588281740236\n","3.3768079501609005 3.2588240048681278 0.1179839452927726 1.0169440346044745\n","3.3767441155582882 3.262496739003101 0.11424737655518732 1.0166824640055931\n","3.3761432071643345 3.260977714866216 0.11516549229811841 1.0181657756798963\n","3.3757844533195369 3.2599298665342946 0.11585458678524248 1.0173146260188124\n","3.3753335862666458 3.2606125852391217 0.11472100102752431 1.0185769800970443\n","3.3748252643900063 3.2618527045429344 0.11297255984707172 1.0171926793640138\n","3.374652540944604 3.2627050440183503 0.11194749692625387 1.0162689926793442\n","3.3745108916506976 3.2626700731817415 0.11184081846895592 1.0147498255616503\n","3.3742962320384184 3.2623024001676759 0.11199383187074258 1.012662947077541\n","3.3739327435782216 3.263557340857608 0.11037540272061357 1.010335844929349\n","3.3735900511021546 3.2624179914314357 0.11117205967071875 1.0100299029651278\n","3.3730561640994767 3.2617070469224982 0.11134911717697864 1.0101568709585291\n","3.3722796489451743 3.2607654110207882 0.11151423792438618 1.0140631438108958\n","3.3709563637686819 3.2592689138636994 0.11168744990498244 1.0184367214732706\n","3.3681790129406042 3.2562039962938822 0.11197501664672212 1.026331440465912\n","3.3658697136331042 3.2571950455243042 0.1086746681088 1.026740127360988\n","3.3614098963397239 3.2618161258328819 0.09959377050684172 1.0277180872458216\n","3.3580533466805189 3.2603402047184331 0.097713141962085623 1.0253520127496834\n","3.3537540823882055 3.2587278250293696 0.095026257358835978 1.0249709684033177\n","3.3497691132663063 3.2550072685087033 0.094761844757603 1.0191746188575972\n","3.3478920798060559 3.2494085906913694 0.098483489114686351 1.0210692778761272\n","3.3466059121781155 3.2478335961879723 0.098772315990143342 1.0202706882766046\n","3.3447885777400082 3.238193213709772 0.10659536403023608 1.018130225778321\n","3.3427405350766053 3.2373119520054185 0.10542858307118678 1.020873671519159\n","3.3424826810591441 3.239057223901801 0.10342545715734316 1.0189296140276554\n","3.3420397716032735 3.2418235069047636 0.10021626469850985 1.0172324311017484\n","3.3410691475089664 3.2413236878789813 0.0997454596299851 1.020071847467491\n","3.3406182541421736 3.2380204983421459 0.10259775580002792 1.0182754109406758\n","3.3403341933956971 3.2392952941004607 0.10103889929523648 1.018520289299467\n","3.3399835526423409 3.2400730275551446 0.099910525087196334 1.0191747845604577\n","3.3394440378238763 3.2391751340320623 0.10026890379181419 1.020058791766754\n","3.3386033244776354 3.2377828516071094 0.10082047287052609 1.0211885886327334\n","3.3379708490051989 3.2354026917059304 0.10256815729926863 1.022547404747863\n","3.3364626438352527 3.2266725930412958 0.10979005079395694 1.0268687991246357\n","3.3361710754697613 3.2272189624602459 0.10895211300951567 1.0299366493147597\n","3.3356678317542641 3.2283348186057257 0.10733301314853837 1.029760092590858\n","3.3351118922332397 3.2248254260368618 0.11028646619637773 1.0289167896066371\n","3.3344928049659326 3.2250575247047726 0.10943528026115998 1.0296787547445259\n","3.3338858352309009 3.2258218885525167 0.10806394667838427 1.0340288808099327\n","3.3331266209011288 3.224099758004729 0.10902686289639968 1.0345773254704995\n","3.3317824600575641 3.2190773202167096 0.11270513984085456 1.037451599702856\n","3.3310738533597877 3.2198807191627044 0.11119313419708338 1.0368633658489506\n","3.3305118709687274 3.2183922029278187 0.11211966804090871 1.0378147690655308\n","3.3296401284149977 3.2137173128936691 0.11592281552132869 1.0418085636086407\n","3.3285910366097249 3.2112911293986461 0.11729990721107859 1.0393028015911479\n","3.3272853163526594 3.212338755360435 0.11494656099222457 1.0388683766982565\n","3.3255126614902131 3.2026701371766753 0.1228425243135377 1.0421078545100952\n","3.3250776935212527 3.1979448148562897 0.12713287866496298 1.0527096415928445\n","3.3240517401809053 3.19438370529271 0.12966803488819528 1.0617596328095402\n","3.3237978816172471 3.1985598068046048 0.12523807481264235 1.055635783626178\n","3.321513194180282 3.1921455159320282 0.12936767824825393 1.0587425010120883\n","3.3209062122254194 3.192004055511183 0.12890215671423635 1.061303205332844\n","3.3201894486827 3.1860242394903344 0.13416520919236563 1.0694957707580295\n","3.319145410186354 3.1838417556201808 0.13530365456617305 1.076921874690351\n","3.3179685040939706 3.1797887272611449 0.13817977683282545 1.0758177901777048\n","3.3174346868420859 3.1789467477904627 0.13848793905162324 1.076752217182367\n","3.31649196008951 3.1762253979649375 0.14026656212457261 1.0741419016209395\n","3.3157400355570692 3.1783609802138977 0.13737905534317138 1.0657417047940672\n","3.3151185007382216 3.1723574725123349 0.1427610282258866 1.0657143442699202\n","3.3146366356893711 3.1746838576256251 0.13995277806374598 1.0653513230634262\n","3.3140396355582848 3.1776484279981956 0.13639120756008907 1.0640681817484616\n","3.3136034605069384 3.1787075558498916 0.13489590465704698 1.0628346819027148\n","3.3130516096687219 3.1821477623722654 0.13090384729645643 1.057310019747295\n","3.3123562453508049 3.186122984906278 0.12623326044452671 1.058593926444724\n","3.311944735156978 3.1860471706958489 0.12589756446112929 1.0581526956259593\n","3.3116037894888355 3.1868576842722445 0.12474610521659119 1.0557563946735316\n","3.3108770258739115 3.1857863441848426 0.12509068168906876 1.0512893800331626\n","3.3099052639227828 3.1825352791668355 0.1273699847559471 1.0473063314486715\n","3.3092172990746036 3.1809854048335797 0.12823189424102369 1.0472625113504928\n","3.3078868357352653 3.1765623437552386 0.13132449198002685 1.0534052168988144\n","3.3067239489396667 3.1786066220677793 0.12811732687188751 1.056777630093873\n","3.3060270186333924 3.1786836596651464 0.127343358968246 1.0559803085500805\n","3.3056214741294405 3.1782601154736505 0.12736135865578985 1.0532187841497072\n","3.3051655376747955 3.1768739968595834 0.12829154081521219 1.050284905595902\n","3.3042693729531223 3.1756763250589541 0.12859304789416831 1.0510603171840474\n","3.3020128360075325 3.1699568689484421 0.1320559670590902 1.050287499916237\n","3.299228308786883 3.160427819073405 0.13880048971347814 1.0479094622140819\n","3.2964638659237537 3.1524981149941715 0.14396575092958208 1.0437778794748565\n","3.2932698412797543 3.1443733211514422 0.14889652012831231 1.0390553099224298\n","3.2883566353412568 3.13208493130941 0.1562717040318467 1.0255777131558188\n","3.2843118870179047 3.1242068856333969 0.16010500138450795 1.0160152412978276\n","3.27927531565252 3.1308083266310835 0.14846698902143637 1.0136231171580854\n","3.2759317234593492 3.128907671004673 0.14702405245467598 1.0125055658814461\n","3.2745339298601621 3.1246637042836825 0.14987022557647972 1.0092195158789532\n","3.2735959037092175 3.1215354070222334 0.15206049668698407 1.0087207379312342\n","3.2712765410357361 3.1165459087634289 0.15473063227230732 1.0098340826408727\n","3.2691704184836441 3.1168349001299593 0.15233551835368458 1.0089457695815025\n","3.2675685550505951 3.1183439946214513 0.14922456042914395 1.0076084726795977\n","3.2653468962895884 3.1160803920846818 0.14926650420490639 1.0040616503099644\n","3.2639742810247183 3.113093311506443 0.15088096951827543 1.0056246286595352\n","3.2633714230390045 3.1130590821116169 0.15031234092738771 1.0066780765745718\n","3.2627214349643103 3.1122037633137212 0.15051767165058913 1.0050893851531757\n","3.2618646897402357 3.1075667438566676 0.15429794588356785 0.9995870242345629\n","3.2613212451378195 3.1081025686714456 0.153218676466374 0.9960554685129933\n","3.2606073721557705 3.1045983267624608 0.1560090453933094 0.9975118680769869\n","3.2600683230297389 3.0998742995546378 0.16019402347510098 1.0002395466334688\n","3.2595408648538435 3.0977655825766264 0.16177528227721716 0.999810472424444\n","3.258907080181892 3.0968980532771297 0.16200902690476213 0.9998773322060598\n","3.2585354026844011 3.0972144451077934 0.16132095757660772 1.0010212702704053\n","3.2583506424014539 3.0932738391157564 0.16507680328569746 1.0035632575765379\n","3.2576098599238734 3.094617125029886 0.16299273489398733 1.0038341662101202\n","3.2571025859308289 3.0909513004079265 0.16615128552290218 1.0008338833799524\n","3.2560211799381884 3.091016022797453 0.16500515714073546 0.9995126100993558\n","3.2543656369945828 3.084654763777221 0.16971087321736186 0.9970909216147704\n","3.25338937916251 3.0845659949136079 0.16882338424890225 0.9979376981727396\n","3.2533832585648548 3.0851270714204553 0.16825618714439938 1.0021446943177863\n","3.25126935618313 3.0859873672805507 0.16528198890257922 0.9988482844085774\n","3.2506208916069328 3.0841880323037123 0.16643285930322035 1.0002177707234226\n","3.2498101541352105 3.08119158418888 0.16861856994633059 1.0005065182575696\n","3.2489854656292581 3.0796509260726039 0.16933453955665434 1.0052842626459637\n","3.2485468842852829 3.0788065471847417 0.1697403371005414 1.0034255262372833\n","3.2483321239621294 3.0792970493419505 0.16903507462017886 1.0039499037665314\n","3.2481875541816096 3.0782576850404633 0.16992986914114622 1.0039237231010816\n","3.2477024584257754 3.0779115801082257 0.16979087831754955 1.003896717012623\n","3.2474158925564987 3.0770951734578875 0.1703207190986111 1.0038222322638555\n","3.24709033723325 3.0754216676446346 0.17166866958861512 1.0050093395282227\n","3.2467101622263024 3.072182003374361 0.17452815885194126 1.0060704477298221\n","3.2465869320214331 3.0707938195721796 0.17579311244925366 1.007050124386721\n","3.2463759547153108 3.0685475364215566 0.17782841829375431 1.007825356339154\n","3.2461231931644976 3.0671047792831114 0.179018413881386 1.0076496307016487\n","3.2459824284002448 3.0625901283153878 0.1833923000848568 1.0068415150159573\n","3.2457706584153549 3.0636780720334484 0.18209258638190642 1.0078861106620418\n","3.2456797369093469 3.0629708699614695 0.18270886694787736 1.0087874834042774\n","3.2455869193617888 3.0613645589325666 0.18422236042922221 1.0088105193832644\n","3.2455004159864695 3.0604600659211925 0.18504035006527689 1.008875588794329\n","3.2453948993362745 3.060143207133625 0.18525169220264939 1.0090988923020203\n","3.2453040491147989 3.0601559669392122 0.18514808217558654 1.009202547234217\n","3.245181701087009 3.0609435673658694 0.18423813372113956 1.0092124697953102\n","3.2450050425678674 3.0615927091481554 0.18341233341971205 1.0092402659242798\n","3.2445596196271018 3.062514789594498 0.18204483003260402 1.0091028836506013\n","3.2435116526251795 3.0624820462370859 0.18102960638809362 1.0087922654948636\n","3.24169693379705 3.0603552612515186 0.18134167254553149 1.0075746329106623\n","3.2386695267447725 3.0556119586885075 0.18305756805626483 1.0039775702578493\n","3.2376746420327791 3.0559146866084821 0.18175995542429704 0.9949429084883332\n","3.2345237799482724 3.0560734428510914 0.17845033709718103 0.9924809926833235\n","3.2322924892915945 3.0514571256578717 0.18083536363372257 0.9933134396874355\n","3.2290938557929105 3.0495881306770496 0.17950572511586077 0.9960085272309717\n","3.2265186060903148 3.0529176849310189 0.17360092115929612 0.9943508264695989\n","3.225111802129712 3.0482847947579939 0.1768270073717183 0.9916274751831594\n","3.22393065783816 3.0494294023532209 0.17450125548493922 0.9930086051922288\n"]}],"source":["N_u = 100 #Total number of data points for 'u'\n","N_f = 10000 #Total number of collocation points \n","\n","# Training data\n","X_f_train, X_u_train, u_train = trainingdata(N_u,N_f)\n","\n","layers = np.array([2,20,20,20,20,20,20,20,20,1]) #8 hidden layers\n","\n","PINN = Sequentialmodel(layers)\n","\n","init_params = PINN.get_weights().numpy()\n","\n","start_time = time.time() \n","\n","# train the model with Scipy L-BFGS optimizer\n","results = scipy.optimize.minimize(fun = PINN.optimizerfunc, \n","                                  x0 = init_params, \n","                                  args=(), \n","                                  method='L-BFGS-B', \n","                                  jac= True,        # If jac is True, fun is assumed to return the gradient along with the objective function\n","                                  callback = PINN.optimizer_callback, \n","                                  options = {'disp': None,\n","                                            'maxcor': 200, \n","                                            'ftol': 1 * np.finfo(float).eps,  #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n","                                            'gtol': 5e-8, \n","                                            'maxfun':  50000, \n","                                            'maxiter': 5000,\n","                                            'iprint': -1,   #print update every 50 iterations\n","                                            'maxls': 50})\n","\n","elapsed = time.time() - start_time                \n","print('Training time: %.2f' % (elapsed))\n","\n","print(results)\n","\n","PINN.set_weights(results.x)\n","\n","''' Model Accuracy ''' \n","u_pred = PINN.evaluate(X_u_test)\n","\n","error_vec = np.linalg.norm((u-u_pred),2)/np.linalg.norm(u,2)        # Relative L2 Norm of the error (Vector)\n","print('Test Error: %.5f'  % (error_vec))\n","\n","u_pred = np.reshape(u_pred,(256,100),order='F')                        # Fortran Style ,stacked column wise!\n","\n","''' Solution Plot '''\n","solutionplot(u_pred,X_u_train,u_train)"]},{"cell_type":"markdown","metadata":{"id":"7SWqKJsg3j9L"},"source":["# Plot of collocation points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Sk4oL6R3j9L"},"outputs":[],"source":["N_u = 100 #Total number of data points for 'u'\n","N_f = 10000 #Total number of collocation points \n","\n","# Training data\n","X_f_train, X_u_train, u_train = trainingdata(N_u,N_f)\n","\n","fig,ax = plt.subplots()\n","\n","plt.plot(X_u_train[:,1], X_u_train[:,0], '*', color = 'red', markersize = 5, label = 'Boundary collocation = 100')\n","plt.plot(X_f_train[:,1], X_f_train[:,0], 'o', markersize = 0.5, label = 'PDE collocation = 10,000')\n","\n","plt.xlabel('t')\n","plt.ylabel('x')\n","plt.title('Collocation points')\n","plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","plt.show()\n","\n","fig.savefig('collocation_points_Burgers.png', dpi = 500, bbox_inches='tight')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"colab":{"name":"OrnsteinUhlenbeck.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}